# IA Local com Mistral (Ollama)

Este projeto Ã© um chatbot simples rodando **localmente** no terminal com o modelo **Mistral**, utilizando a API do [Ollama](https://ollama.com/). Ideal para testar IA de forma offline e com privacidade.

---

# IA Local com Mistral rodando via Ollama

Este projeto Ã© uma interface de terminal para interagir com o modelo de linguagem Mistral localmente utilizando o Ollama.

## ðŸ“ Clonando o repositÃ³rio

```bash
git clone https://github.com/cavaliieri/local-ai.git
cd local-ai
```

## ðŸ› ï¸ InstalaÃ§Ã£o

1. Certifique-se de que o [Ollama](https://ollama.com/download) esteja instalado e em execuÃ§Ã£o em sua mÃ¡quina.
2. Execute o comando para baixar o modelo Mistral:
```bash
ollama run mistral
```
3. Instale as dependÃªncias Python:
```bash
pip install ollama
```

## â–¶ï¸ Como usar

Execute o script principal no terminal:
```bash
python app.py
```
O terminal pedirÃ¡ seu nome e vocÃª poderÃ¡ interagir com a IA local.

Digite `sair` para encerrar o chat.

## ðŸ’¡ Exemplo de uso

```bash
====== Bem vindo a IA  =========

Digite sua pergunta:
> Qual a capital da FranÃ§a?
IA: A capital da FranÃ§a Ã© Paris.
```

## ðŸ§  Modelo Utilizado

- **Mistral 7B** rodando localmente via Ollama.

## ðŸ§± Estrutura do Projeto

```
local-ai/
â”‚
â”œâ”€â”€ app.py               # Script principal com o chat no terminal
â”œâ”€â”€ README.md            # Este arquivo
â””â”€â”€ requirements.txt     # DependÃªncias do projeto (opcional)
```

## ðŸ§° Tecnologias Utilizadas

- Python 3.x
- Ollama
- Mistral 7B
- Terminal (modo texto)

## ðŸ“¬ Contato

Entre em contato comigo:

- GitHub: [@cavaliieri](https://github.com/cavaliieri)
- Email: lbcavalieri@gmail.com
